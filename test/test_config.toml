[llm]
provider = "ollama"

[llm.ollama]
endpoint = "http://localhost:11434"
model = "tinyllama"
